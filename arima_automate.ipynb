{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to create an ARIMA Model\n",
    "\n",
    "Step 1 — Check stationarity: If a time series has a trend or seasonality component, it must be made stationary before we can use ARIMA to forecast. \n",
    "\n",
    "Step 2 — Difference: If the time series is not stationary, it needs to be stationarized through differencing. Take the first difference, then check for stationarity. Take as many differences as it takes. Make sure you check seasonal differencing as well.\n",
    "\n",
    "Step 3 — Filter out a validation sample: This will be used to validate how accurate our model is. Use train test validation split to achieve this.\n",
    "\n",
    "Step 4 — Select AR and MA terms: Use the ACF and PACF to decide whether to include an AR term(s), MA term(s), or both.\n",
    "\n",
    "Step 5 — Build the model: Build the model and set the number of periods to forecast to N (depends on your needs).\n",
    "\n",
    "Step 6 — Validate model: Compare the predicted values to the actuals in the validation sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check stationarity\n",
    "\n",
    "Stationary time series is when the mean and variance are constant over time. It is easier to predict when the series is stationary.‘Stationarity’ is one of the most important concepts you will come across when working with time series data. A stationary series is one in which the properties – mean, variance and covariance, do not vary with time.\n",
    "\n",
    "6. Why does Time Series(TS) need to be stationary?\n",
    "A. It is because of the following reasons:\n",
    "a) If a TS has a particular behavior over a time interval, then there's a high probability that over a different interval, it will have same behavior, provided TS is stationary. This helps in forecasting accurately.\n",
    "b) Theories & Mathematical formulas ae more mature & easier to apply for as TS which is stationary.\n",
    "\n",
    "\n",
    "Types of Stationarity\n",
    "Let us understand the different types of stationarities and how to interpret the results of the above tests.\n",
    "\n",
    "- Strict Stationary: A strict stationary series satisfies the mathematical definition of a stationary process. For a strict stationary series, the mean, variance and covariance are not the function of time. The aim is to convert a non-stationary series into a strict stationary series for making predictions.\n",
    "- Trend Stationary: A series that has no unit root but exhibits a trend is referred to as a trend stationary series. Once the trend is removed, the resulting series will be strict stationary. The KPSS test classifies a series as stationary on the absence of unit root. This means that the series can be strict stationary or trend stationary.\n",
    "- Difference Stationary: A time series that can be made strict stationary by differencing falls under difference stationary. ADF test is also known as a difference stationarity test.\n",
    "\n",
    "### Methods to Check Stationarity:\n",
    "\n",
    " #### Visual test:\n",
    " we can plot the data and determine if the properties of the series are changing with time or not.\n",
    " \n",
    " Rolling mean can show us whether the data is stationary or not.\n",
    " which will let us know whether the data has constant mean and std at different time intervals \n",
    " \n",
    " https://towardsdatascience.com/time-series-analysis-resampling-shifting-and-rolling-f5664ddef77e#:~:text=Rolling%20is%20a%20very%20useful,course%2C%20rolls%20through%20the%20data.\n",
    " \n",
    " How to Decompose Time Series Data into Trend and Seasonality\n",
    " \n",
    " Time series decomposition involves thinking of a series as a combination of level, trend, seasonality, and noise components.\n",
    "\n",
    "Decomposition provides a useful abstract model for thinking about time series generally and for better understanding problems during time series analysis and forecasting.\n",
    "\n",
    "the ACF plot is also useful for identifying non-stationary time series. For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly. Also, for non-stationary data, the value of r1 is often large and positive.\n",
    "ACF wil show many lags as significant because in some way or the other they are related to the original time series.\n",
    "we use lags to know the percentage change in sales.\n",
    "lag(1) determines the percentage change in sales of present day with respect to the previous day.\n",
    "\n",
    "In general, the \"partial\" correlation between two variables is the amount of correlation between them which is not explained by their mutual correlations with a specified set of other variables. For example, if we are regressing a variable Y on other variables X1, X2, and X3, the partial correlation between Y and X3 is the amount of correlation between Y and X3 that is not explained by their common correlations with X1 and X2. This partial correlation can be computed as the square root of the reduction in variance that is achieved by adding X3 to the regression of Y on X1 and X2.\n",
    "\n",
    "\n",
    "PACF is a partial auto-correlation function. Basically instead of finding correlations of present with lags like ACF, it finds correlation of the residuals (which remains after removing the effects which are already explained by the earlier lag(s)) with the next lag value hence ‘partial’ and not ‘complete’ as we remove already found variations before we find the next correlation.\n",
    "\n",
    "A partial autocorrelation is the amount of correlation between a variable and a lag of itself that is not explained by correlations at all lower-order-lags. The autocorrelation of a time series Y at lag 1 is the coefficient of correlation between Yt and Yt-1, which is presumably also the correlation between Yt-1 and Yt-2. But if Yt is correlated with Yt-1, and Yt-1 is equally correlated with Yt-2, then we should also expect to find correlation between Yt and Yt-2. In fact, the amount of correlation we should expect at lag 2 is precisely the square of the lag-1 correlation. Thus, the correlation at lag 1 \"propagates\" to lag 2 and presumably to higher-order lags. The partial autocorrelation at lag 2 is therefore the difference between the actual correlation at lag 2 and the expected correlation due to the propagation of correlation at lag 1.\n",
    "\n",
    "\n",
    "After plotting the ACF plot we move to Partial Autocorrelation Function plots (PACF). A partial autocorrelation is a summary of the relationship between an observation in a time series with observations at prior time steps with the relationships of intervening observations removed(it removes the.\n",
    "\n",
    "If the PACF plot drops off at lag n(sudden drop), then use an AR(n) model and if the drop in PACF is more gradual(step by step or dropping slowly) then we use the MA term.\n",
    "\n",
    "\n",
    "<b>statistical test:</b>\n",
    "\n",
    "ADF (Augmented Dickey Fuller) Test\n",
    "The Dickey Fuller test is one of the most popular statistical tests. It can be used to determine the presence of unit root in the series, and hence help us understand if the series is stationary or not. The null and alternate hypothesis of this test are:\n",
    "\n",
    "- Null Hypothesis: The series has a unit root (value of a =1)\n",
    "\n",
    "- Alternate Hypothesis: The series has no unit root.\n",
    "\n",
    "If we fail to reject the null hypothesis, we can say that the series is non-stationary. This means that the series can be linear or difference stationary.\n",
    "\n",
    "If the test statistic is less than the critical value, we can reject the null hypothesis (aka the series is stationary). When the test statistic is greater than the critical value, we fail to reject the null hypothesis (which means the series is not stationary).\n",
    "\n",
    "In our above example, the test statistic > critical value, which implies that the series is not stationary. This confirms our original observation which we initially saw in the visual test.\n",
    "\n",
    " \n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2018/09/non-stationary-time-series-python/#:~:text=Test%20for%20stationarity%3A%20If%20the,the%20series%20is%20not%20stationary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differencing\n",
    "\n",
    "The second (and most important) step in fitting an ARIMA model is the determination of the order of differencing needed to stationarize the series. Normally, the correct amount of differencing is the lowest order of differencing that yields a time series which fluctuates around a well-defined mean value and whose autocorrelation function (ACF) plot decays fairly rapidly to zero, either from above or below. If the series still exhibits a long-term trend, or otherwise lacks a tendency to return to its mean value, or if its autocorrelations are are positive out to a high number of lags (e.g., 10 or more), then it needs a higher order of differencing. We will designate this as our \"first rule of identifying ARIMA models\" :\n",
    "- Rule 1: If the series has positive autocorrelations out to a high number of lags, then it probably needs a higher order of differencing.\n",
    "\n",
    "\n",
    "Differencing tends to introduce negative correlation: if the series initially shows strong positive autocorrelation, then a nonseasonal difference will reduce the autocorrelation and perhaps even drive the lag-1 autocorrelation to a negative value. If you apply a second nonseasonal difference (which is occasionally necessary), the lag-1 autocorrelation will be driven even further in the negative direction.\n",
    "\n",
    "If the lag-1 autocorrelation is zero or even negative, then the series does not need further differencing.  You should resist the urge to difference it anyway just because you don't see any pattern in the autocorrelations!  One of the most common errors in ARIMA modeling is to \"overdifference\" the series and end up adding extra AR or MA terms to undo the damage.   If the lag-1 autocorrelation is more negative than -0.5 (and theoretically a negative lag-1 autocorrelation should never be greater than 0.5 in magnitude), this may mean the series has been overdifferenced. The time series plot of an overdifferenced series may look quite random at first glance, but if you look closer you will see a pattern of excessive changes in sign from one observation to the next--i.e., up-down-up-down, etc. \n",
    "\n",
    "- Rule 2: If the lag-1 autocorrelation is zero or negative, or the autocorrelations are all small and patternless, then the series does not need a higher order of  differencing. If the lag-1 autocorrelation is -0.5 or more negative, the series may be overdifferenced.  BEWARE OF OVERDIFFERENCING!!\n",
    "\n",
    "A common \"rookie error\" in ARIMA modeling is to apply an extra order of differencing because the current autocorrelation plot does not show much of a pattern. If it doesn't, that's good, not bad! Another symptom of possible overdifferencing is an increase in the standard deviation, rather than a reduction, when the order of differencing is increased.\n",
    "\n",
    "- Rule 3: The optimal order of differencing is often the order of differencing at which the standard deviation is lowest.\n",
    "\n",
    "In the Forecasting procedure in Statgraphics, you can find the order of differencing that minimizes the standard deviation by fitting ARIMA models with various orders of differencing and no coefficients other than a constant. For example, if you fit an ARIMA(0,0,0) model with constant, an ARIMA(0,1,0) model with constant, and an ARIMA(0,2,0) model with constant, then the RMSE's will be equal to the standard deviations of the original series with 0, 1, and 2 orders of nonseasonal differencing, respectively.\n",
    "\n",
    "The first two rules do not always unambiguously determine the \"correct\" order of differencing. We will see later that \"mild underdifferencing\" can be compensated for by adding AR terms to the model, while \"mild overdifferencing\" can be compensated for by adding MA terms instead. In some cases, there may be two different models which fit the data almost equally well: a model that uses 0 or 1 order of differencing together with AR terms, versus a model that uses the next higher order of differencing together with MA terms. In trying to choose between two such models that use different orders of differencing, you may need to ask what assumption you are most comfortable making about the degree of nonstationarity in the original series--i.e., the extent to which it does or doesn't have fixed mean and/or a constant average trend.\n",
    "\n",
    "- Rule 4: A model with no orders of differencing assumes that the original series is stationary (mean-reverting). A model with one order of differencing assumes that the original series has a constant average trend (e.g. a random walk or SES-type model, with or without growth). A model with two orders of total differencing assumes that the original series has a time-varying trend (e.g. a random trend or LES-type model).\n",
    "\n",
    "\n",
    "- Rule 5: A model with no orders of differencing normally includes a constant term (which allows for a non-zero mean value). A model with two orders of total differencing normally does not include a constant term. In a model with one order of total differencing, a constant term should be included if the series has a non-zero average trend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation\n",
    "Transformations are used to stabilize the non-constant variance of a series. Common transformation methods include power transform, square root, and log transform. Let’s do a quick log transform and differencing on our air passenger dataset:\n",
    "Transformations such as logarithms can help to stabilise the variance of a time series. Differencing can help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First-order differencing addresses linear trends, and employs the transformation zi = yi – yi-1. Second-order differencing addresses quadratic trends and employs a first-order difference of a first-order difference, namely zi = (yi – yi-1) – (yi-1 – yi-2), which is equivalent to zi = yi – 2yi-1+ yi-2.\n",
    "\n",
    "Seasonal differencing\n",
    "A seasonal difference is the difference between an observation and the previous observation from the same season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoregressive integrated moving average (ARIMA) process (aka a Box-Jenkins process) adds differencing to an ARMA process. An ARMA(p,q) process with d-order differencing is called an ARIMA(p.d,q) process. Thus, for example, an ARIMA(2,1,0) process is an AR(2) process with first-order differencing.\n",
    "\n",
    "It is important not to over-difference since this can cause you to use an incorrect model. Some rules-of-thumb indicating that you may have differenced too many times are:\n",
    "\n",
    "The autocorrelation of a differenced series is less than -.5\n",
    "Differencing increases the variance\n",
    "An AR(p) or MA(q) process has a unit root if the sum of the non-constant coefficients is 1.\n",
    "\n",
    "Additional rules-of-thumb:\n",
    "\n",
    "If an AR(p) process has a unit root then the level of differencing should be increased\n",
    "If an MA(q) process has a unit root then the level of differencing should be decreased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that column A contains a time series of size n starting in cell A1. Now suppose we want to place the 1st order differences (of size n-1) in column B starting in cell B2, the 2nd order differences (of size n-2) in column C starting in cell C3, and so on with the 7th order differences (of size n-7) in column H starting in cell H8, then the formulas that need to be used in these starting cells are as follows:\n",
    "\n",
    "- B2: A2-A1\n",
    "- C3: A3-2*A2+A1\n",
    "- D4: A4-3*A3+3*A2-A1\n",
    "- E5: A5-4*A4+6*A3-4*A2+A1\n",
    "- F6: A6-5*A5+10*A4-10*A3+5*A2-A1\n",
    "- G7: A7-6*A6+15*A5-20*A4+15*A3-6*A2+A1\n",
    "- H8: A8-7*A7+21*A6-35*A5+35*A4-21*A3+7*A2-A1\n",
    "\n",
    "\n",
    "https://people.duke.edu/~rnau/411arim2.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arima assumptions:\n",
    "\n",
    "Normally in an ARIMA model, we make use of either the AR term or the MA term. We use both of these terms only on rare occasions. We use the ACF plot to decide which one of these terms we would use for our time series\n",
    "- If there is a Positive autocorrelation at lag 1 then we use the AR model\n",
    "- If there is a Negative autocorrelation at lag 1 then we use the MA model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR MODEL\n",
    "\n",
    "Autoregressive component: A purely AR model forecasts only using a combination of the past values sorta like linear regression where the number of AR terms used is directly proportional to the number of previous periods taken into consideration for the forecasting.\n",
    "\n",
    "Auto regressive (AR) process , a time series is said to be AR when present value of the time series can be obtained using previous values of the same time series i.e the present value is weighted average of its past values. Stock prices and global temperature rise can be thought of as an AR processes.\n",
    "\n",
    "Use AR terms in the model when the\n",
    "- ACF plots show autocorrelation decaying towards zero\n",
    "- PACF plot cuts off quickly towards zero\n",
    "- ACF of a stationary series shows positive at lag-1\n",
    "\n",
    "https://online.stat.psu.edu/stat462/node/188/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MA MODEL\n",
    "Moving Averages: Random jumps in the time series plot whose effect is felt in two or more consecutive periods. These jumps represent the error calculated in our ARIMA model and represent what the MA component would lag for. A purely MA model would smooth out these sudden jumps like the exponential smoothing method.\n",
    "Use MA terms in the model when the model is\n",
    "- Negatively Autocorrelated at Lag — 1\n",
    "- ACF that drops sharply after a few lags\n",
    "- PACF decreases more gradually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated component\n",
    "Integrated component: This component comes into action when the time series is not stationary. The number of times we have to difference the series to make it stationary is the parameter(i-term) for the integrated component\n",
    "We can represent our model as ARIMA(ar-term, ma-term, i-term)\n",
    "Finding the correct model is an iterative process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to find value of P & Q for ARIMA ?\n",
    "A. We need to take help of ACF(Auto Correlation Function) & PACF(Partial Auto Correlation Function) plots. ACF & PACF graphs are used to find value of P & Q for ARIMA. We need to check, for which value in x-axis, graph line drops to 0 in y-axis for 1st time.\n",
    "From PACF(at y=0), get P\n",
    "From ACF(at y=0), get Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After creating a model, you need to interpret the summary stats of the model to check whether we can use the model.\n",
    "https://medium.com/analytics-vidhya/interpreting-arma-model-results-in-statsmodels-for-absolute-beginners-a4d22253ad1c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to choose the model based on ACF AND PACF\n",
    "https://online.stat.psu.edu/stat510/book/export/html/665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
